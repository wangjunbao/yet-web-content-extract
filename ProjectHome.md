网站正文提取在信息检索系统中占有非常重要的地位。很多网页在有用的正文信息之外往往还会包含很多的噪音信息，比如导航条、推广链接、版权信息、脚本、样式等等，同时由于设计、编码习惯等各方面的原因，加之有不少网站甚至存在html标记错误，我们不可能使用一种方法来对所有的网站进行提取。

常见的正文提取算法有：

基于DOM树，这种算法需要对网站HTML建立DOM树，然后对之进行遍历递归，去除相应的噪音信息之后再从剩余的节点中进行选择。这种方法由于要建立DOM树，时间/空间复杂度均较高，同时DOM树的建立对HTML是否良构具有相当高的依赖，毕竟现在不按常规出牌的网站实在太多。

基于标记窗，先提取页面的标题title，标签，h1-h6，组成标记窗等，然后对内容进行分词，再计算内容词语与标记窗词语之间的距离。这种算法对学术类的网站或许比较准确，但是，想想吧，内容抓取、分词、计算词语距离的时间/空间复杂度。

基于经验，例如，提取标签中包含content,body等字样的页面实体，再取出它的子元素进行处理。这种方法的优点是可以直接去掉大部噪声元素，大多数情况下比较准确，但是需要建立大量的经验库。在只抓小范围网站内容的时候可以使用这一方案。

当然，还有诸如数据挖掘等算法，但是一来复杂程度较高，二来性能也较差，对简单的需求来说并不实用。所以我实现了一个基于内容分布的算法。原理很简单：哪一块文字最密集，哪一块就是网站的正文。这种算法的优势是简单、快速，缺点是对于正文全是图片的网站、或者链接密集的网站没有识别能力。

实现方案：1、去除重复的换行，\n,\n\r,\n\s\r等。2、去除脚本、样式、不需要的meta信息；3、去除不具有实际意义的标签，div/table/form等；4、使用\n对内容进行分割；5、取出最大的连续的块。

DEMO：http://htmlconv.sinaapp.com/get_content.php

使用示例：
//实例化类，参数1直接传入html，参数2，指定网页地址
$extract = new Extract('', $url);
//忽略连续链接的阀值
$extract->ignore\_link($ignore\_link);
//允许连续空行数
$extract->space\_step($ignore\_line);
//提取网页标题
$title = $extract->extract\_title(true);
//获取正文
$content = $extract->get\_content();